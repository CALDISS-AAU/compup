{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing and Machine Learning\n",
    "\n",
    "This is a Jupyter Notebook running with Python, which contains a step-by-step example of some of the most basic and useful tools in Natural Language Processing (NLP). Explanations for each cell is given partially in text, but relies on the content of the workshop presentation.\n",
    "\n",
    "You do not need to know about programming in Python in order to run this notebook. There are places where the code can be modified without such knowledge. This is noted in the text, and will be explained thoroughly in the workshop.\n",
    "\n",
    "The notebook was initially developed by Johannes Bjerva (jbjerva@cs.aau.dk / https://bjerva.github.io) as part of the Digital Literacy programme. It was further modified by Ross Deans Kristensen-McLachlan (rdkm@cas.au.dk) for the CompUp workshop at AAU. If you have any questions, do not hesitate to contact me (Ross).\n",
    "\n",
    "\n",
    "**Note: This Notebook uses data from the OffensEval 2020 shared task on hate speech detection (Zampieri et al, 2020). Be aware that examples of hatespeech from this dataset are used in the cells below, and is not in any way meant as an endorsement of such utterances or behaviour.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Jupyter Notebook Cell\n",
    "\n",
    "If you aren't familiar with Jupyter notebooks, the main thing you need to know is:\n",
    "\n",
    "* Each \"block\" below is known as a **cell**\n",
    "* In order to execute / run a cell, simply select the cell by clicking, and input **shift+Enter** on your keyboard\n",
    "* The code in the cell will then execute (can be instant, or take up to 30 seconds depending on your computer), and the output will be displayed below the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "The first step is to make sure that we have access to all libraries and models that we need.\n",
    "In particular, these are:\n",
    "* Spacy (for NLP processing)\n",
    "* Spacy's Danish model\n",
    "* Scikit-learn (for machine learning classifiers)\n",
    "* Matplotlib (for plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the libraries we need\n",
    "# python system tools\n",
    "import os\n",
    "\n",
    "# Loading the Danish Spacy models\n",
    "import spacy\n",
    "from spacy.lang.da.stop_words import STOP_WORDS\n",
    "nlp = spacy.load(\"da_core_news_lg\")\n",
    "\n",
    "# data analysis tools\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# some tools for 'Classical Machine Learning'\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# For visualisations\n",
    "from IPython.display import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import hist\n",
    "%matplotlib inline\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which tools we have access to in this NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the most informative featurs - again, don't worry too much about the details\n",
    "def show_most_informative_features(vectorizer, classifier, n=20):\n",
    "    \"\"\"\n",
    "    Return the most informative features from a classifier, i.e. the 'strongest' predictors.\n",
    "    \n",
    "    vectorizer:\n",
    "        A vectorizer defined by the user, e.g. 'CountVectorizer'\n",
    "    classifier:\n",
    "        A classifier defined by the user, e.g. 'MultinomialNB'\n",
    "    n:\n",
    "        Number of features to display, defaults to 20\n",
    "        \n",
    "    \"\"\"\n",
    "    # Get feature names and coefficients\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(classifier.coef_[0], feature_names))\n",
    "    # Get ordered labels\n",
    "    labels = sorted(set(train_Y))\n",
    "    # Select top n results, where n is function argument\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    # Pretty print columns showing most informative features\n",
    "    print(f\"{labels[0]}\\t\\t\\t\\t{labels[1]}\\n\")\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "            print(\"%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We're going to work with Danish data from the OffensEval 2020 shared task on hatespeech detection (Zampieri et al. 2020).\n",
    "https://sites.google.com/site/offensevalsharedtask/home\n",
    "\n",
    "If you want to play around with your own data, this is relatively straight forward if you can convert it to a .tsv file with the following format:\n",
    "\n",
    "```<ID>\\t<TEXT>\\t<LABEL> ```\n",
    "\n",
    "If you require something more complex, feel free to reach out to me after the workshop at: rdkm@cas.au.dk\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variable with directory name\n",
    "directory = \"../dkhate/\"\n",
    "\n",
    "# create variables for train and test data files\n",
    "train_file = os.path.join(directory, \"offenseval-da-training-v1.tsv\")\n",
    "test_file = os.path.join(directory, \"offenseval-da-test-v1.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now read the training and test data from the given files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = pd.read_csv(train_file, sep=\"\\t\").dropna()\n",
    "test_data = pd.read_csv(test_file, sep=\"\\t\").dropna()\n",
    "\n",
    "# get labels\n",
    "train_Y = train_data[\"subtask_a\"]\n",
    "test_Y = test_data[\"subtask_a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see how many examples we have to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"Number of training instances:\", len(train_data))\n",
    "print(\"Number of test instances:\", len(test_data))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are almost 3000 training instances and about 300 test instances.\n",
    "\n",
    "The labels are either \"NOT\" for \"Not Hatespeech\" or \"OFF\" for hatespeech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at a single example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = train_data.iloc[52] # Choosing a nice example\n",
    "print(\"\\nTraining example:\")\n",
    "print(\"\\tID:\\t\", example[0])\n",
    "print(\"\\tText:\\t\", example[1])\n",
    "print(\"\\tLabel:\\t\", example[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the label balance in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[\"subtask_a\"].value_counts())\n",
    "train_data[\"subtask_a\"].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, most tweets are not offensive.\n",
    "We will return to this data after going through a standard NLP pipeline with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Naïve Approach - Vectorizing and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization. What is it and why are all the cool kids talking about it?\n",
    "\n",
    "Essentially, vectorization is the process whereby textual or visual data is 'transformed' into some kind of numerical representation. One of the easiest ways to do this is to simple count how often individual features appear in a document.\n",
    "\n",
    "Take the following text:\n",
    "\n",
    "<center> <i>My father’s family name being Pirrip, and my Christian name Philip, my infant tongue could make of both names nothing longer or more explicit than Pip. So, I called myself Pip, and came to be called Pip.</i> </center><br>\n",
    "\n",
    "We can convert this into the following vector\n",
    "\n",
    "| and | be | being | both | called | came | christian | could | explicit | family | father | i | infant | longer | make | more | my | myself | name | names | nothing | of | or | philip | pip | pirrip | s | so | than | to | tongue|\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |  --- |\n",
    "| 2 | 1 | 1 | 1 | 2 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 3 | 1 | 2 | 1 | 1 | 1 | 1 | 1 | 3 | 1 | 1 | 1 | 1 | 1 | 1 |\n",
    "\n",
    "<br>\n",
    "Our textual data is hence reduced to a jumbled-up 'vector' of numbers, known somewhat quaintly as a <i>bag-of-words</i>.\n",
    "<br>\n",
    "<br>\n",
    "To do this in practice, we first need to create our vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range = (1,2),    # unigrams and bigrams (1 word and 2 word units)\n",
    "                             stop_words = STOP_WORDS, # why use stopwords?\n",
    "                             lowercase = True,       # why use lowercase?\n",
    "                             max_df = 0.95,          # remove very common words\n",
    "                             min_df = 0.01,          # remove very rare words\n",
    "                             max_features = 500)     # keep only top 500 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vectorizer is then used to turn all of our documents into a vector of numbers, instead of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we do it for our training data...\n",
    "training_features = vectorizer.fit_transform(train_data[\"tweet\"])\n",
    "#... then we do it for our test data\n",
    "test_features = vectorizer.transform(test_data[\"tweet\"])\n",
    "# We can also create a list of the feature names. \n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Q: What are the first 20 features that are picked out by the CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying and predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to 'fit' the classifier to our data. This means that the classifier takes our data and finds correlations between features and labels.\n",
    "\n",
    "These correlations are then the *model* that the classifier learns about our data. This model can then be used to predict the label for new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression().fit(training_features, train_Y)\n",
    "\n",
    "predictions = classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What are the predictions for the first 20 examples of the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the model, in order to see which features are most informative when trying to predict a label. \n",
    "\n",
    "To do this, we can use the ```show_most_informative_features``` function that I defined earlier - how convenient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_most_informative_features(vectorizer, classifier, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computer has now learned a model of how our data behaves. But is it accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"../img/confusionMatrix.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True Positive Rate => Recall  => Sensitivity => (TP / TP + FN)\n",
    "\n",
    "Sensitivity tells us what proportion of the positive class got correctly classified. <br>\n",
    "i.e number of sick people correctly identified.\n",
    "\n",
    "#### True negative rate => Specificity => (TN / TN + FP) \n",
    "Specificity tells us what proportion of the negative class got correctly classified. <br>\n",
    "i.e. the proportion of healthy people who were correctly identified.\n",
    "\n",
    "#### False negative rate => (FN / TP + FN)\n",
    "proportion of the positive class got incorrectly classified by the classifier\n",
    "\n",
    "#### False positive rate = (FP / TN + FP) = 1 - Specificity\n",
    "proportion of the negative class got incorrectly classified by the classifier.\n",
    "\n",
    "#### Precision =>  (TP / TP + FP)\n",
    "patients that we correctly identify having COVID out of all the patients actually having it <br>\n",
    "ie. ratio of true positives to all positives\n",
    "\n",
    "#### F1 => 2(PR / P + R)\n",
    "Harmonic mean of precision and recall, useful where both precision and recall are important\n",
    "\n",
    "#### Accuracy => (TP+TN)/(TP+FP+FN+TN)\n",
    "Ratio of correct classifications, relative to total dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Thankfully, libraries like ```sklearn``` come with a range of tools that are useful for evaluating models.\n",
    "\n",
    "One way to do this, is to use a confusion matrix, similar to what you see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(classifier, training_features, train_Y,\n",
    "                              cmap=plt.cm.Blues, labels=[\"NOT\", \"OFF\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do some quick calculations, in order to assess just how well our model performs.\n",
    "\n",
    "NB: Slightly different terminology but Recall is the same as Sensitivity in the confusion matrix above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_metrics = metrics.classification_report(test_Y, predictions)\n",
    "print(classifier_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A More Sophisticated Approach\n",
    "We will now go through how we can go from a given input text (X) and a given output (Y), to a model which can take *any* input X and **predict** which Y value it has.\n",
    "\n",
    "The first step is to obtain the NLP analysis we want, using the SpaCy NLP pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the current pipeline\n",
    "for tool in nlp.pipeline:\n",
    "    print(tool[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can change this sentence to anything you want, and see what output you get in the steps below!\n",
    "example = \"Frøken Jensen bor i Aalborg, tæt på Limfjorden\"\n",
    "\n",
    "# With Spacy getting NLP tools to analyze a sentence is as easy as:\n",
    "doc = nlp(example)\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple white-space based tokenization goes wrong\n",
    "for token in example.split():\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Spacy's tokenization gets it right\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Note that the automatic solution gives the wrong lemma for \"tæt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the POS for \"tæt\" is correct, even though the lemma is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print('{0:10.10} {1}'.format(token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print('{0:10.10} {1}'.format(token.text, token.dep_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corresponds to the dependency parse below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Note that the NER tool is successfully able to recognise that the full name of the entity is \"Dr. Andersen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note that we loop over doc.ents rather than simply 'doc'\n",
    "for ent in doc.ents:\n",
    "    print('{0:15.15} {1}'.format(ent.text, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A (very) simple Machine Learning pipeline using NLP\n",
    "\n",
    "We will now look at combining these things into a full pipeline for the HateSpeech detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features(data, train=True):\n",
    "    '''Extract required features for each sentence in the data set'''\n",
    "    X = []\n",
    "    for sentence in data:\n",
    "        curr_X = []\n",
    "        doc = nlp(sentence)\n",
    "        for token in doc:\n",
    "            if \"tokens\" in features:\n",
    "                curr_X.append(token.text)\n",
    "            if \"lemmas\" in features:\n",
    "                curr_X.append(token.lemma_)\n",
    "            if \"parse\" in features:\n",
    "                curr_X.append(token.dep_)\n",
    "            if \"pos\" in features:\n",
    "                curr_X.append(token.pos_)\n",
    "            if \"token+pos\" in features:\n",
    "                curr_X.append(token.text + token.pos_)\n",
    "                \n",
    "        if \"ner\" in features:\n",
    "            for ent in doc.ents:\n",
    "                curr_X.append(ent.label_)\n",
    "                \n",
    "                \n",
    "        X.append(\" \".join(curr_X))\n",
    "    if train:\n",
    "        X_counts = count_vect.fit_transform(X)\n",
    "    else:\n",
    "        X_counts = count_vect.transform(X)\n",
    "        \n",
    "    return X_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the below cell might take some time, as running the SpaCy NLP tool on the entire training and test set is time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Settings ###\n",
    "\n",
    "# Ignore features with a lower frequency than this:\n",
    "minimum_feature_frequency = 10 \n",
    "\n",
    "# You can edit which features to use by commenting in or out these list items\n",
    "# Add a '#' in front of a line (like with, e.g., #\"lemmas\",), in order to remove a feature\n",
    "features = [\n",
    "    #\"pos\",\n",
    "    #\"parse\",\n",
    "    \"ner\",\n",
    "    \"lemmas\",\n",
    "    #\"tokens\",\n",
    "    #\"token+pos\",\n",
    "] \n",
    "\n",
    "### End of Settings ###\n",
    "\n",
    "# Note that feature extraction can take some time!\n",
    "count_vect = CountVectorizer(min_df=minimum_feature_frequency)\n",
    "train_X = get_features(train_data[\"tweet\"])\n",
    "test_X = get_features(test_data[\"tweet\"], train=False)\n",
    "\n",
    "print(\"The amount of features is:\", train_X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now select and train/fit a classifier. The classifiers are from scikit-learn, and can be replaced with other options from: \n",
    "\n",
    "https://scikit-learn.org/stable/supervised_learning.html#supervised-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the classifier to the training data\n",
    "classifier = LogisticRegression().fit(train_X, train_Y)\n",
    "predictions = classifier.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well we do on data which we *have already observed*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(classifier, train_X, train_Y,\n",
    "                              cmap=plt.cm.Blues, labels=[\"NOT\", \"OFF\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the full classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "classifier_metrics = metrics.classification_report(test_Y, predictions)\n",
    "print(classifier_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis - opening the \"Black Box\"\n",
    "\n",
    "Let's get a general impression of what mistakes the model makes on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to check a single case\n",
    "\n",
    "sentence = test_data.iloc[42]\n",
    "print(\"\\nTest example:\")\n",
    "print(\"\\tID:\\t\", sentence[0])\n",
    "print(\"\\tText:\\t\", sentence[1])\n",
    "print(\"\\tLabel:\\t\", sentence[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the model predicts:\n",
    "\n",
    "prediction = predictions[42]\n",
    "print(\"The model predicts:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a broader picture, let's investigate several cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's get several error cases:\n",
    "\n",
    "n_errors = 5\n",
    "n = 0\n",
    "\n",
    "print(\"\\n\")\n",
    "for idx, (pred, gold) in enumerate(zip(predictions, test_Y)):\n",
    "    if pred != gold:\n",
    "        print(f\"Model incorrectly predicts '{pred}' for the sentence: {test_data.iloc[idx]['tweet']}\")\n",
    "        n += 1\n",
    "    if n >= n_errors:\n",
    "        break\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Some are \"obviously\" wrong:\n",
    "* Model incorrectly predicted 'NOT' for the sentence: NED MED SVENSKEN!\n",
    "* Model incorrectly predicted 'OFF' for the sentence: Ja tak. Og jeg kører selv mc.\n",
    "\n",
    "\n",
    "Some are perhaps debatable and might be offensive in certain contexts:\n",
    "* Model incorrectly predicted 'NOT' for the sentence: @USER ryger du hash. ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout discussions\n",
    "\n",
    "- Besides feature engineering and choice of classifier algorithm, what else might improve performance?\n",
    "- What potential problems are there with the data, which might impact performance?\n",
    "- Besides their practical use for classification tasks, for what else might you use the NLP techniques outlined here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other use cases, beyond 'solving' a task\n",
    "\n",
    "* Finding all Named Entities in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities = []\n",
    "for sentence in test_data[\"tweet\"]:\n",
    "    doc = nlp(sentence)\n",
    "    for ent in doc.ents:\n",
    "        named_entities.append(ent.text)\n",
    "\n",
    "for entity in named_entities[:10]:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most NEs make sense, but there are some issues.\n",
    "\n",
    "Let's count and visualise the most frequent Named Entities below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_frequency = 3 # The minimum occurrence of a named entity\n",
    "data = Counter(named_entities)\n",
    "names = list(i for i in data.keys() if data [i] >= min_frequency)\n",
    "values = list(i for i in data.values() if i >= min_frequency)\n",
    "\n",
    "matplotlib.pyplot.bar(names, values)\n",
    "p = plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
